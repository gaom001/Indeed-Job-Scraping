{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biomedical Data position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# pages reference    \n",
    "pages=[0,10,20,30,40,50,60,70,80]\n",
    "\n",
    "# Create dataframe for hodling all info\n",
    "df_t = pd.DataFrame()    \n",
    "postion_title=[]\n",
    "Position_web=[]\n",
    "Position_date=[]\n",
    "Position_company=[]\n",
    "Position_city=[]\n",
    "Position_state=[]\n",
    "Position_summary=[] \n",
    "\n",
    "# loop each page and scrape each position element by using beautifulSoup \n",
    "# Key words: Math+Teacher       Location: Texas\n",
    "\n",
    "for page in pages:        \n",
    "    url = f'https://www.indeed.com/jobs?q=Math+Teacher&l=Texas&sort=date&start={page}'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Scraping each elements\n",
    "    results = soup.find_all(\"h2\", class_='title')\n",
    "    for result in results:    \n",
    "        title=result.a.text.replace('\\n', ' ')\n",
    "        postion_title.append(title)\n",
    "    \n",
    "        link=result.a['href']\n",
    "        web=f\"https://www.indeed.com{link}\" \n",
    "        Position_web.append(web)\n",
    "       \n",
    "    dates = soup.find_all(class_='date')\n",
    "    for date in dates:\n",
    "        update=date.text\n",
    "        Position_date.append(update)\n",
    "    \n",
    "    companies = soup.find_all(class_='company')\n",
    "    for company in companies:\n",
    "        name=company.text.replace('\\n', ' ')\n",
    "        Position_company.append(name)\n",
    "    \n",
    "    locations = soup.find_all(class_='location accessible-contrast-color-location')\n",
    "    for location in locations:\n",
    "        cityname=location.text.split(\",\")[0]\n",
    "        Position_city.append(cityname) \n",
    "        statename=location.text.split(\",\")[-1][0:3]\n",
    "        Position_state.append(statename) \n",
    "    \n",
    "    summaries = soup.find_all(class_='summary')\n",
    "    for summary in summaries:\n",
    "        description=summary.text.replace('\\n', ' ')\n",
    "        Position_summary.append(description)   \n",
    "\n",
    "# add each element into empty dataframe          \n",
    "df_t[\"Postion\"]=postion_title\n",
    "df_t[\"Date\"]=Position_date\n",
    "df_t[\"Company\"]=Position_company\n",
    "df_t[\"State\"]=Position_state\n",
    "df_t[\"City\"]=Position_city\n",
    "df_t[\"Link\"]=Position_web\n",
    "df_t[\"Description\"]=Position_summary\n",
    "\n",
    "# Export dataframe. \n",
    "df_t.to_csv(\"math teacher.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
